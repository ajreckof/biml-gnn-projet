\documentclass{article}

\usepackage{amsmath,amsthm}     
\usepackage{graphicx}     
\usepackage{hyperref} 
\usepackage{url}
\usepackage{amsfonts} 
\usepackage[margin=1in]{geometry}
\usepackage{float}
\usepackage{multicol, multirow}
\usepackage{arydshln}
\usepackage{subcaption}
\usepackage[french]{babel}
\usepackage{adjustbox}



\allowdisplaybreaks

\makeatletter
\@addtoreset{footnote}{page}
\makeatother

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}

\renewcommand{\arraystretch}{1.5}

\title{BIML GNN : Prédiction de lien \\
\footnotesize{VGAE et autres joyeusetés}}
\author{Bonhoure Timothé, Martinez Christophe}                      %%%% your final manuscript.

\maketitle
\tableofcontents
\section*{Abstract}
\newpage

\section{Méthode}

Dans le cadre de ce projet, nous avons décidé de développer notre propre décodeur pour la prédiction de liens. Ce décodeur est composé de deux couches linéaires séparées par une fonction d'activation ReLu. Pour évaluer son efficacité, nous avons mis en place deux modèles :
\begin{itemize}
    \item Un VGAE (Variational Graph Autoencoder) avec un encodeur composé d'une couche GCNConv suivie de deux couches GCNConv pour encoder la moyenne et l'écart type dans l'espace latent.
    \item Un GAE (Graph Autoencoder) composé d'une seule couche GCNConv.
\end{itemize}
Pour la préparation des données nous utilisons la méthode suivante :
\begin{verbatim}
torch_geometric.transforms.RandomLinkSplit(is_undirected=True, split_labels=True, num_val=0)
\end{verbatim}
Cette méthode nous permet de générer des jeux d’entraînement et de test de liens existants dans les données d'origine (liens positifs) et non existants dans les données d'origine (liens négatifs). Elle nous permet aussi de définir que ces liens sont non orientés et que donc pour chaque paire d’indices formant le lien, le premier indice est inférieur au deuxième.
Nous avons opté pour l'utilisation de l'optimiseur Adam avec un taux d'apprentissage de 0,01 et un terme de régularisation (\texttt{weight\_decay}) de \(5e^{-4}\).
Tous les modèles présents auront une taille de sortie de l'espace latent de 32.

Dans l'ensemble de nos expérimentations, nous fournissons aux modèles de prédiction des données comprenant la latitude, la longitude et le pays (converti en un code numérique). En plus de cela, nous avons décidé d'explorer l'ajout de l'information sur le degré de chaque nœud et de comparer les résultats.
Les degrés des nœuds sont déterminés en utilisant notre connaissance du graphe réel. Cependant, lorsque nous envisageons d'intégrer un nouvel aéroport à notre base de données, il devient envisageable d'estimer le degré potentiel du nœud associé à cet aéroport en se basant sur les flux d'arrivées et de départs des avions de cet aéroport. Ainsi, la réalisation de ce test nous semble pertinente et justifiée.


\section{Dropout}
\label{sec:dropout}
\subsection{Méthode}
Nous avons essayé d'améliorer le modèle VGAE en y incorporant une couche de dropout entre la première et la deuxième couche. 
Nous avons laissé la valeur de dropout à la valeur par défaut soit 0.5. 
Les modèles ont été entraînés sur 2000 epochs. 
Ce processus a été répété 60 fois pour obtenir des statistiques sur l'apprentissage.
Le processus a été réalisé à la fois dans la situation ou le degré est inconnu et dans la situation ou il est connu

\subsection{Résultats}
Les résultats au bout des 2000 epochs ont été compilé dans le tableau \ref{tab:dropout}.
L'évolution au cours de l'apprentissage de l'AUC et la précision moyenne ont, elles, été représenté sur la figure \ref{fig:dropout}.
Les résultats montrent sans équivoque que la présence de cette couche réduit énormément les performances du modèles sans réduire de manière suffisante l'écart-type et cela dans les deux situations.
En effet, en regardant les courbes de la figure \ref{fig:dropout} on remarque que seul les instances les plus performantes de VGAE avec du dropout arrive à dépasser la médiane des instances de VGAE sans dropout.
Pour la suite des tentatives on ne conservera donc pas le dropout.

\begin{table}[H]
    \captionsetup{justification=centering}
    \adjustbox{center}{
        \begin{tabular}{|c|*{2}{c:c:c|}} 
            \hline
            \multirow{2}{*}{modèle} &\multicolumn{3}{|c|}{Avec degré} & \multicolumn{3}{|c|}{Sans degré} \\
            \cline{2-7}
            & \footnotesize{AUC} & \footnotesize{AP} & \footnotesize{temps} &\footnotesize{AUC} & \footnotesize{AP} & \footnotesize{temps} \\
            \hline
            VGAE              & \textbf{.823}(\textbf{.015}) & \textbf{75.6\%}(2.2\%) & \textbf{107.8s}(\textbf{4.9s}) & \textbf{.830}(.023)     & \textbf{75.5\%}(2.8\%) & \textbf{110.9s}(\textbf{6.2s})\\
            VGAE avec dropout & .785(.017)                   & 70.6\%(\textbf{1.9\%}) & 109.3s(5.3s)                   & .781(\textbf{.021})     & 70.0\%(\textbf{2.6\%}) & 112.0s(9.345)\\
            \hline
        \end{tabular}
    }
    \caption{Résultats de l'utilisation du dropout. \\ \footnotesize Dans chaque case est indiquée la moyenne et l'écart-type au format : moyenne(écart-type)}
    \label{tab:dropout}
\end{table}


\section{Décodeur}
\subsection{Méthode}
Pour améliorer le VGAE nous avons décidé d’entraîner en plus de l'encodeur un décodeur. 
Le décodeur que nous avons entraîné est constitué de deux couches linéaires.
La fonction d'activation entre les deux couches est ReLu.
En sortie du décodeur la fonction d'activation est une sigmoïde
Les modèles ont été entraîné sur 2000 epochs. 
Ce processus a été répété 60 fois pour obtenir des statistiques sur l'apprentissage.
Le processus a été réalisé à la fois dans la situation ou le degré est inconnu et dans la situation ou il est connu.

\subsection{Résultat}
Les résultats au bout des 2000 epochs ont été compilé dans le tableau \ref{tab:decoder}.
L'évolution au cours de l'apprentissage de l'AUC et la précision moyenne ont, elles, été représenté sur la figure \ref{fig:decoder}.
Les résultats montre que l'utilisation du décodeur augmente très fortement les performances au détriment d'un écart type plus important. 
Sur la figure \ref{fig:decoder} on voit que l'augmentation d'écart-type est dû à une petite portion des instances ayant une chute extrêmement forte (passant en dessous des instances sans décodeur).
\begin{table}[H]
    \captionsetup{justification=centering}
    \adjustbox{center}{
        \begin{tabular}{|c|*{2}{c:c:c|}} 
            \hline
            \multirow{2}{*}{modèle} &\multicolumn{3}{|c|}{Avec degré} & \multicolumn{3}{|c|}{Sans degré} \\
            \cline{2-7}
            & \footnotesize{AUC} & \footnotesize{AP} & \footnotesize{temps} &\footnotesize{AUC} & \footnotesize{AP} & \footnotesize{temps} \\
            \hline
            VGAE avec décodeur & \textbf{.960}(.047) & \textbf{95.8\%}(5.2\%) & 118.4s(6.4s)                   & \textbf{.955}(.061) & \textbf{94.9\%}(7.0\%) & 122.8s(7.2s)\\
            VGAE               & .823(\textbf{.015}) & 75.6\%(\textbf{2.2\%}) & \textbf{107.8s}(\textbf{4.9s}) & .830(\textbf{.023}) & 75.5\%(\textbf{2.8\%}) & \textbf{110.9s}(\textbf{6.2s})\\
            \hline
        \end{tabular}
    }
    \caption{Résultats de l'utilisation d'un décodeur. \\ \footnotesize Dans chaque case est indiquée la moyenne et l'écart-type au format : moyenne(écart-type)}
    \label{tab:decoder}
\end{table}

\section{Dropout en présence du décodeur}
\subsection{Méthode}
On réutilise la couche de dropout introduit dans la partie \ref{sec:dropout} et on la teste sur le vgae avec décodeur.
En particulier on souhaite voir si son utilisation ne permettrais pas de réduire l'écart-type important introduit par le décodeur sans pour autant perdre trop en performance. 
Les modèles ont été entraînés sur 2000 epochs. 
Ce processus a été répété 60 fois pour obtenir des statistiques sur l'apprentissage.
Le processus a été réalisé à la fois dans la situation ou le degré est inconnu et dans la situation ou il est connu.

\subsection{Résultat}
Les résultats au bout des 2000 epochs ont été compilé dans le tableau \ref{tab:decoder}.
L'évolution au cours de l'apprentissage de l'écart type de l'AUC et de la précision moyenne ont, elles, été représenté sur la figure \ref{fig:decoder}.
On remarque que l'écart-type est fortement réduit grâce aux dropout (divisé par 2 dans la situation ou le degré est connu et par 10 lorsque celui ci ne l'est pas).
En plus de cela on peut remarquer que la moyenne elle-même augment en effet les performances hautes sont peu impacté et les performances basses ont été fortement augmenté.

\begin{table}[H]
    \captionsetup{justification=centering}
    \adjustbox{center}{
        \begin{tabular}{|c|*{2}{c:c:c|}} 
            \hline
            \multirow{2}{*}{modèle} &\multicolumn{3}{|c|}{Avec degré} & \multicolumn{3}{|c|}{Sans degré} \\
            \cline{2-7}
            & \footnotesize{AUC} & \footnotesize{AP} & \footnotesize{temps} &\footnotesize{AUC} & \footnotesize{AP} & \footnotesize{temps} \\
            \hline
            VGAE avec décodeur            & .960(.047)                   & 95.8\%(5.2\%)                   & \textbf{118.4s}(\textbf{6.4s}) & .955(.061)                   & 94.9\%(7.0\%)                   & \textbf{122.8s}(\textbf{7.2s})\\
            VGAE avec décodeur et dropout & \textbf{.962}(\textbf{.028}) & \textbf{96.0\%}(\textbf{3.4\%}) & 120.0s(6.6s)                   & \textbf{.959}(\textbf{.006}) & \textbf{95.2\%}(\textbf{0.7\%}) & \textbf{122.8s}(8.5s)\\
            \hline
        \end{tabular}
    }
    \caption{Résultats de l'utilisation du dropout en presence d'un décodeur. \\ \footnotesize Dans chaque case est indiquée la moyenne et l'écart-type au format : moyenne(écart-type)}
    \label{tab:dropout_decoder}
\end{table}

\section{Ablation study}
Dans cette section on va vérifier que chacune des spécificités de notre modèle est bien nécessaire. 
La première spécificité est l'utilisation du dropout que l'on a justifié à la section précédente. 
La deuxième spécificité est l'utilisation d'un décodeur différent de celui par défaut. 
La dernière spécificité est l'utilisation de VGAE.
\subsection{Utilisation d'un décodeur}
Nous avons déjà vu que le décodeur avait un effet positif. Cependant, nous n'avons pas testé son efficacité en présence de dropout. Cependant, il n'y a pas besoin de refaire des calculs en effet. Avec le décodeur le dropout est meilleure et de plus sans le décodeur le dropout est moins bien. Ainsi : 
\[
    \text{VGAE avec dropout} < \text{VGAE} < \text{VGAE avec décodeur} < \text{VGAE avec dropout et décodeur}
\]
Donc le décodeur reste efficace en présence de dropout il est même encore plus efficace en présence de dropout.
\subsection{Utilisation d'un VGAE}
Nous allons maintenant vérifier la pertinence de l'utilisation d'un VGAE en le comparant avec un simple GAE. 
Les modèles ont été entraînés sur 2000 epochs. 
Ce processus a été répété 60 fois pour obtenir des statistiques sur l'apprentissage.
Le processus a été réalisé à la fois dans la situation ou le degré est inconnu et dans la situation ou il est connu.

Les résultats ont été compilés dans le tableau~\ref{tab:vgae}. 
On remarque que dans le cas ou le degré est connu Un simple GAE va avoir de meilleure performance que le VGAE.
 De plus le GAE mais 2 fois moins de temps pour réaliser le même nombre d'epochs. 
 En revanche dans le cas plus compliqué ou le degré n'est pas connu, 
 le VGAE produit de meilleures performances au détriment d'un plus long temps de calcul et d'un plus grand écart-type. 
 Sur la figure~\ref{fig:vgae} on voit que VGAE atteint de meilleures performances dans la situation ou le degré n'est pas connu,
 mais que certaines instances ont de très mauvaises performances faisant baisser les performances moyennes.
\begin{table}[H]
    \captionsetup{justification=centering}
    \adjustbox{center}{
        \begin{tabular}{|c|*{2}{c:c:c|}} 
            \hline
            \multirow{2}{*}{modèle} &\multicolumn{3}{|c|}{Avec Degré} & \multicolumn{3}{|c|}{Sans Degré} \\
            \cline{2-7}
            & \footnotesize{AUC} & \footnotesize{AP} & \footnotesize{temps} &\footnotesize{AUC} & \footnotesize{AP} & \footnotesize{temps} \\
            \hline
            GAE avec décodeur             & \textbf{.968}(\textbf{.007}) & \textbf{96.9\%}(\textbf{0.6\%}) & \textbf{64.5s}(\textbf{3.5s}) & .946(\textbf{.004}) & 93.5\%(\textbf{0.5\%}) & \textbf{67.1s}(\textbf{3.8s})\\
            VGAE avec décodeur et dropout & .962(.028)                   & 96.0\%(3.4\%)                   & 120.0s(6.6s)                  & \textbf{.959}(.006) & \textbf{95.2\%}(0.7\%) & 122.8s(8.5s)\\
            \hline
        \end{tabular}
    }
    \caption{Résultats de l'utilisation d'un GAE au lieu du VGAE. \\ \footnotesize Dans chaque case est indiquée la moyenne et l'écart-type au format : moyenne(écart-type)}
    \label{tab:vgae}
\end{table}
\subsection{Conclusion}
Ainsi on peut voir que le décodeur et l'utilisation de dropout apporte réellement quelque chose à notre modèle.
Cependant, dans la situation ou le degré des nœuds est connu alors un simple GAE avec le décodeur permet d'avoir de meilleur résultat avec une meilleure stabilité qu'avec un VGAE.
En revanche dans le cas ou le degré est inconnu le VGAE reste plus performant.

\section{Reconstruction de graphe}
\subsection{Méthode}
Nous avons choisi de tester nos modèles d'une manière plus visuelle, en cherchant à reconstruire un graphique représentant les liaisons aériennes entre les différents aéroports de notre ensemble de données. Pour ce faire, nous avons suivi une préparation des données similaire à celle décrite dans la section précédente. Nous avons ensuite entraîné nos modèles en utilisant une base de liens positifs pour l'entraînement. Par la suite, nous avons généré des graphiques en encodant les mêmes liens positifs et en décodant un ensemble de liens provenant de l'ensemble de données d'origine, ainsi qu'un nombre équivalent de liens négatifs. Nous avons ensuite enregistré plusieurs métriques, notamment le nombre de liens correctement prédits, le nombre de liens faussement prédits (qui n'existent pas dans nos données), le nombre de liens manqués (qui existent dans nos données, mais ont été rejetés par le modèle), et le nombre total de liens rejetés.
Nous avons utilisé 4 modèles :
\begin{itemize}
    \item Un GAE avec un encodeur formé d’une seule couche de convolution.
    \item Un VGAE sans notre décodeur.
    \item Un VGAE avec notre décodeur.
    \item Un même GAE mais avec notre décodeur.
\end{itemize}

Chaque modèle a été testé sur un total de 27 093 liens possibles, comprenant 13 547 liens positifs et 13 546 liens négatifs. L'objectif était de couper ce jeu de liens en deux en conservant les liens les plus plausibles pour le modèle. Pour ce faire, chaque modèle attribue un score entre 0 et 1 à chacun des 27 093 liens. Ensuite, à l'aide de la méthode \texttt{torch.quantile(z, 0.5)}, nous déterminons un seuil de score pour ne conserver que les 13 546 ± 1 liens ayant un score supérieur au seuil. Ce sont ces liens qui seront considérés comme prédis positivement par le modèle. Dans nos résultats, les pourcentages associés aux liens corrects et faux sont basés sur le nombre de liens considérés positifs par le modèle (Corrects + Faux) et le pourcentage de liens manqués par rapport au total de liens positifs réels (13 547).
La prédiction est faite sur les données de \textbf{latitude}, \textbf{longitude},  et \textbf{pays} de chaque nœud.\newline
\newline
Les liens en noir sont les liens correctement prédis.\newline
Les liens en rouge sont les liens faussement prédis.\newline
Les liens en vert sont les liens manqués.

\subsection{Résultats}

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Modèle & Min & Max & Moyenne & Écart type & Seuil\\
        \hline
        GAE & 0 & 1 & 0.75929 & 0.42746 & 1\\
        VGAE & 0 & 1 & 0.71422 & 0.45044 & 1\\
        GAE avec décodeur & 0 & 0.99980 & 0.49157 & 0.39727 & 0.54739\\
        VGAE avec décodeur & 0 & 1 & 0.56486 & 0.42113 & 0.75337\\
        \hline
    \end{tabular}
    \caption{Statistiques des scores établies par les modèles}
    \label{tab:statistiques_scores}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{|c|*{3}{c:c|}c|}
        \hline
        Modèle & \multicolumn{2}{c|}{Corrects} & \multicolumn{2}{c|}{Faux} & \multicolumn{2}{c|}{Manqués} & Rejetés\\
        \hline
        GAE & 13298 & 64.7\% & 7257 & 35.3\% & 249 & 1.84\% & 6538\\
        VGAE & \textbf{13381} & 71.0\% & 5473 & 29.0\% & \textbf{166} & \textbf{1.23\%} & 8239\\
        GAE avec décodeur & 11804 & 87.1\% & 1743 & 12.9\% & 1743 & 12.9\% & \textbf{13546}\\
        VGAE avec décodeur & 12199 & \textbf{90.0\%} & \textbf{1348} & \textbf{10\%} & 1348 & 10\% & \textbf{13546}\\
         \hline
    \end{tabular}
    \caption{Résultats de prédictions. voir figure \ref{fig:fig_graphe_GAE} \ref{fig:fig_graphe_GAE_with_decodeur} \ref{fig:fig_graphe_VGAE} \ref{fig:fig_graphe_VGAE_with_decodeur}}
    \label{tab:resultats_reconstruction}
\end{table}

Il est notable que les modèles sans décodeur ont manqué beaucoup moins de liens par rapport aux modèles avec décodeur. Cependant, cette amélioration s'accompagne d'une plus grande acceptation de faux liens qui auraient dû être rejetés. Cette tendance s'explique en partie par la manière dont ces modèles attribuent des scores, sans faire de distinction nette entre les liens potentiellement positifs et les liens considérés comme certainement positifs.

\subsection{Résultats avec connaissance du degré}
Lors de nos tests, nous avons exploré l'incorporation des degrés de chaque nœud pour la prédiction de liens. Nous avons alors cherché à tester de manière plus visuelle nos modèles.

\begin{table}[H]
    \centering
    \begin{tabular}{|c|c|c|c|c|c|}
        \hline
        Modèle & Min & Max & Moyenne & Écart type & Seuil\\
        \hline
        GAE & 0 & 1 & 0.73815 & 0.43963 & 1\\
        VGAE & 0 & 1 & 0.75625 & 0.41074 & 1\\
        GAE avec décodeur & 0 & 1 & 0.49266 & 0.45323 & 0.5414\\
        VGAE avec décodeur & 0 & 1 & 0.51289 & 0.43108 & 0.5917\\
        \hline
    \end{tabular}
    \caption{Statistiques des scores établies par les modèles en tenant compte du degré des nœuds}
    \label{tab:statistiques_scores_avec_degre}
\end{table}

\begin{table}[H]
    \centering
    \begin{tabular}{|c|*{3}{c:c|}c|}
        \hline
        Modèle & \multicolumn{2}{c|}{Corrects} & \multicolumn{2}{c|}{Faux} & \multicolumn{2}{c|}{Manqués} & Rejetés\\
        \hline
        GAE & \textbf{13239} & 66.2\% & 6745 & 33.8\% & 308 & 2.27\% & 6797\\
        VGAE & 11665 & 74.5\% & 3995 & 25.5\% & 1882 & 13.9\% & 11427\\
        GAE avec décodeur & 12044 & 88.9\% & 1501 & 11.1\% & 1502 & 11.1\% & 12044\\
        VGAE avec décodeur & 12535 & \textbf{92.5\%} & \textbf{1011} & \textbf{7.5\%} & \textbf{1012} & \textbf{7.5\%} & \textbf{13547}\\
         \hline
    \end{tabular}
    \caption{Résultats de prédictions en tenant compte du degré des nœuds}
    \label{tab:resultats_reconstruction_avec_degre}
\end{table}

Nous pouvons constater que tous les modèles obtiennent de meilleures performances lorsque le degré des nœuds est pris en compte. Notamment, dans ce contexte, le modèle GAE avec décodeur surpasse en performance le modèle VGAE avec décodeur.

\newpage
\section{Annexes}
Ce situe ici les graphiques qui nous ont paru les plus pertinents pour la discussion des résultats. Cependant, d'autres graphiques ont été réalisés et peuvent être retrouvé dans le dossier graphique.
\subsection{Performance}
\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}{0.45\textwidth}
      \includegraphics[width=\textwidth]{graphics/APs_degree_dropout_cinf.svg.pdf}
      \centering
      \caption{Précision moyenne au cours de l'apprentissage\\ avec l'information de degré}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
      \includegraphics[width=\textwidth]{graphics/APs_no_degree_dropout_cinf.svg.pdf}
      \centering
      \caption{précision moyenne au cours de l'apprentissage\\ sans l'information de degré}
    \end{subfigure}
    
    \begin{subfigure}{0.45\textwidth}
      \includegraphics[width=\textwidth]{graphics/AUCs_degree_dropout_cinf.svg.pdf}
      \centering
      \caption{AUC ROC au cours de l'apprentissage\\ avec l'information de degré}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
      \includegraphics[width=\textwidth]{graphics/AUCs_no_degree_dropout_cinf.svg.pdf}
      \centering
      \caption{AUC ROC au cours de l'apprentissage\\ sans l'information de degré}
    \end{subfigure}
    \caption{Évolution de l'AUC et de la précision moyenne au cours de l'apprentissage dans les deux cas (degré connu ou inconnu) entre avec et sans dropout}
    \label{fig:dropout}
\end{figure}
\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}{0.45\textwidth}
      \includegraphics[width=\textwidth]{graphics/APs_degree_decoder_cinf.svg.pdf}
      \centering
      \caption{précision moyenne au cours de l'apprentissage\\ avec l'information de degré}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
      \includegraphics[width=\textwidth]{graphics/APs_no_degree_decoder_cinf.svg.pdf}
      \centering
      \caption{précision moyenne au cours de l'apprentissage\\ sans l'information de degré}
    \end{subfigure}
    
    \begin{subfigure}{0.45\textwidth}
      \includegraphics[width=\textwidth]{graphics/AUCs_degree_decoder_cinf.svg.pdf}
      \centering
      \caption{AUC ROC au cours de l'apprentissage\\ avec l'information de degré}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
      \includegraphics[width=\textwidth]{graphics/AUCs_no_degree_decoder_cinf.svg.pdf}
      \centering
      \caption{AUC ROC au cours de l'apprentissage\\ sans l'information de degré}
    \end{subfigure}
    \caption{Évolution de l'AUC et de la précision moyenne au cours de l'apprentissage dans les deux cas (degré connu ou inconnu) entre VGAE simple et VGAE avec décodeur}
    \label{fig:decoder}
\end{figure}

\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}{0.45\textwidth}
      \includegraphics[width=\textwidth]{graphics/APs_degree_decoder_dropout_std.svg.pdf}
      \centering
      \caption{précision moyenne au cours de l'apprentissage\\ avec l'information de degré}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
      \includegraphics[width=\textwidth]{graphics/APs_no_degree_decoder_dropout_std.svg.pdf}
      \centering
      \caption{précision moyenne au cours de l'apprentissage\\ sans l'information de degré}
    \end{subfigure}
    
    \begin{subfigure}{0.45\textwidth}
      \includegraphics[width=\textwidth]{graphics/AUCs_degree_decoder_dropout_std.svg.pdf}
      \centering
      \caption{AUC ROC au cours de l'apprentissage\\ avec l'information de degré}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
      \includegraphics[width=\textwidth]{graphics/AUCs_no_degree_decoder_dropout_std.svg.pdf}
      \centering
      \caption{AUC ROC au cours de l'apprentissage\\ sans l'information de degré}
    \end{subfigure}
    \caption{Évolution de l'AUC et de la précision moyenne au cours de l'apprentissage dans les deux cas (degré connu ou inconnu) entre avec et sans dropout en présence du décodeur}
    \label{fig:dropout_decoder}
\end{figure}

\begin{figure}[H]
    \centering
    \captionsetup{justification=centering}
    \begin{subfigure}{0.45\textwidth}
      \includegraphics[width=\textwidth]{graphics/APs_degree_vgae_cinf.svg.pdf}
      \centering
      \caption{précision moyenne au cours de l'apprentissage\\ avec l'information de degré}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
      \includegraphics[width=\textwidth]{graphics/APs_no_degree_vgae_cinf.svg.pdf}
      \centering
      \caption{précision moyenne au cours de l'apprentissage\\ sans l'information de degré}
    \end{subfigure}
    
    \begin{subfigure}{0.45\textwidth}
      \includegraphics[width=\textwidth]{graphics/AUCs_degree_vgae_cinf.svg.pdf}
      \centering
      \caption{AUC ROC au cours de l'apprentissage\\ avec l'information de degré}
    \end{subfigure}
    \begin{subfigure}{0.45\textwidth}
      \includegraphics[width=\textwidth]{graphics/AUCs_no_degree_vgae_cinf.svg.pdf}
      \centering
      \caption{AUC ROC au cours de l'apprentissage\\ sans l'information de degré}
    \end{subfigure}
    \caption{Évolution de l'AUC et de la précision moyenne au cours de l'apprentissage dans les deux cas (degré connu ou inconnu) entre VGAE et GAE}
    \label{fig:vgae}
\end{figure}


\subsection{Reconstruction de graphe}

Les liens en noir sont les liens correctement prédis.\newline
Les liens en rouge sont les liens faussement prédis.\newline
Les liens en vert sont les liens manqués.

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{../GCN.png}
    \caption{Reconstruction du graphe avec le GAE}
    \label{fig:fig_graphe_GAE}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{../GCNwithDecoder.png}
    \caption{Reconstruction du graphe avec le GAE muni de notre décodeur}
    \label{fig:fig_graphe_GAE_with_decodeur}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{../withoutDecoder.png}
    \caption{Reconstruction du graphe avec le VGAE}
    \label{fig:fig_graphe_VGAE}
\end{figure}

\begin{figure}[H]
    \centering
    \includegraphics[width=1\linewidth]{../withDecoder.png}
    \caption{Reconstruction du graphe avec le VGAE muni de notre décodeur}
    \label{fig:fig_graphe_VGAE_with_decodeur}
\end{figure}

\end{document}
